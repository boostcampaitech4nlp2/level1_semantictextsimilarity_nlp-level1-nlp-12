{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c5d1c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93847cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76808284",
   "metadata": {},
   "source": [
    "* 유의어로 교체(Synonym Replacement, SR): 문장에서 랜덤으로 stop words가 아닌 n 개의 단어들을 선택해 임의로 선택한 동의어들 중 하나로 바꾸는 기법.\n",
    "\n",
    "* 랜덤 삽입(Random Insertion, RI): 문장 내에서 stop word를 제외한 나머지 단어들 중에서, 랜덤으로 선택한 단어의 동의어를 임의로 정한다. 그리고 동의어를 문장 내 임의의 자리에 넣는걸 n번 반복한다.\n",
    "\n",
    "* 랜덤 교체(Random Swap, RS): 무작위로 문장 내에서 두 단어를 선택하고 위치를 바꾼다. 이것도 n번 반복\n",
    "\n",
    "* 랜덤 삭제(Random Deletion, RD): 확률 p를 통해 문장 내에 있는 각 단어들을 랜덤하게 삭제한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "494077ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아버지가 안방에 들어가신다\n"
     ]
    }
   ],
   "source": [
    "# EDA 사용방법\n",
    "from koeda import EDA\n",
    "\n",
    "eda = EDA(\n",
    "    morpheme_analyzer=\"Okt\", alpha_sr=0.3, alpha_ri=0.3, alpha_rs=0.2, prob_rd=0.1\n",
    ")\n",
    "\n",
    "text = \"아버지가 방에 들어가신다\"\n",
    "\n",
    "result = eda(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c1106b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어머니가 ! 집을 나가신다\n"
     ]
    }
   ],
   "source": [
    "# AEDA 사용방법\n",
    "from koeda import AEDA\n",
    "\n",
    "aeda = AEDA(\n",
    "    morpheme_analyzer=\"Okt\", punc_ratio=0.3, punctuations=[\".\", \",\", \"!\", \"?\", \";\", \":\"]\n",
    ")\n",
    "\n",
    "text = \"어머니가 집을 나가신다\"\n",
    "\n",
    "result = aeda(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7eba15",
   "metadata": {},
   "source": [
    "sentence_1을 바꿀지, sentence_2를 바꿀지, sentence_1,2 모두 바꿀지? <br/>\n",
    "5점 경우는 2700개의 데이터를 추가로 만들고 싶다 <br/>\n",
    "문장은 사이클로 돌리고 sent 1,2,(1,2)는 random으로 선택하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "dbeda6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from koeda import AEDA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "def get_preprocessed_label(df):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        df (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i, \"preprocessed_label\"] = round(df.loc[i, \"label\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def concat_AEDA_sent(df_label, aug_nums):\n",
    "    \"\"\"라벨 x만 있는 데이터프레임에 aug_nums 만큼 AEAD된 데이터를 추가\n",
    "\n",
    "    Args:\n",
    "        df_label (DataFrame): 라벨 x만 있는 데이터프레임\n",
    "        aug_nums (int): 증강하고자 하는 수\n",
    "    Returns:\n",
    "        AEDA가 추가된 라벨 x의 데이터프레임\n",
    "    \"\"\"\n",
    "    np.random.seed(12)\n",
    "    aug_idx = np.random.randint(0, 3, size=aug_nums)\n",
    "    dataset_idx = 0\n",
    "    aug_df_label = copy.deepcopy(df_label)\n",
    "\n",
    "    aeda = AEDA(\n",
    "        morpheme_analyzer=\"Okt\",\n",
    "        punc_ratio=0.3,\n",
    "        punctuations=[\".\", \",\", \"!\", \"?\", \";\", \":\"],\n",
    "    )\n",
    "\n",
    "    for i in range(aug_nums):\n",
    "        if dataset_idx >= len(df_label):\n",
    "            dataset_idx = 0\n",
    "\n",
    "        origin_id = df_label.iloc[dataset_idx][\"id\"]\n",
    "        origin_source = df_label.iloc[dataset_idx][\"source\"]\n",
    "        origin_sentence_1 = df_label.iloc[dataset_idx][\"sentence_1\"]\n",
    "        origin_sentence_2 = df_label.iloc[dataset_idx][\"sentence_2\"]\n",
    "        origin_label = df_label.iloc[dataset_idx][\"label\"]\n",
    "        origin_binarylabel = df_label.iloc[dataset_idx][\"binary-label\"]\n",
    "        origin_preprocessed_label = df_label.iloc[dataset_idx][\"preprocessed_label\"]\n",
    "\n",
    "        if aug_idx[i] == 0:  # sent_1만 aug\n",
    "            sent = aug_df_label.iloc[dataset_idx][\"sentence_1\"]\n",
    "            aug_sent = aeda(sent)\n",
    "            new_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"id\": [origin_id],\n",
    "                    \"source\": [origin_source],\n",
    "                    \"sentence_1\": [aug_sent],\n",
    "                    \"sentence_2\": [origin_sentence_2],\n",
    "                    \"label\": [origin_label],\n",
    "                    \"binary-label\": [origin_binarylabel],\n",
    "                    \"preprocessed_label\": [origin_preprocessed_label],\n",
    "                }\n",
    "            )\n",
    "            aug_df_label = pd.concat([aug_df_label, new_df], ignore_index=True)\n",
    "\n",
    "        elif aug_idx[i] == 1:  # sent_2만 aug\n",
    "            sent = aug_df_label.iloc[dataset_idx][\"sentence_2\"]\n",
    "            aug_sent = aeda(sent)\n",
    "            new_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"id\": [origin_id],\n",
    "                    \"source\": [origin_source],\n",
    "                    \"sentence_1\": [origin_sentence_1],\n",
    "                    \"sentence_2\": [aug_sent],\n",
    "                    \"label\": [origin_label],\n",
    "                    \"binary-label\": [origin_binarylabel],\n",
    "                    \"preprocessed_label\": [origin_preprocessed_label],\n",
    "                }\n",
    "            )\n",
    "            aug_df_label = pd.concat([aug_df_label, new_df], ignore_index=True)\n",
    "\n",
    "        else:  # sent_1과 2를 모두 aug\n",
    "            sent_1 = aug_df_label.iloc[dataset_idx][\"sentence_1\"]\n",
    "            sent_2 = aug_df_label.iloc[dataset_idx][\"sentence_2\"]\n",
    "            aug_sent_1 = aeda(sent_1)\n",
    "            aug_sent_2 = aeda(sent_2)\n",
    "            new_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"id\": [origin_id],\n",
    "                    \"source\": [origin_source],\n",
    "                    \"sentence_1\": [aug_sent_1],\n",
    "                    \"sentence_2\": [aug_sent_2],\n",
    "                    \"label\": [origin_label],\n",
    "                    \"binary-label\": [origin_binarylabel],\n",
    "                    \"preprocessed_label\": [origin_preprocessed_label],\n",
    "                }\n",
    "            )\n",
    "            aug_df_label = pd.concat([aug_df_label, new_df], ignore_index=True)\n",
    "\n",
    "        dataset_idx += 1\n",
    "\n",
    "    return aug_df_label\n",
    "\n",
    "\n",
    "def AEDA_data():\n",
    "\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    train = get_preprocessed_label(train)\n",
    "\n",
    "    train_0 = train[train[\"preprocessed_label\"] == 0].reset_index(drop=True)\n",
    "    train_1 = train[train[\"preprocessed_label\"] == 1].reset_index(drop=True)\n",
    "    train_2 = train[train[\"preprocessed_label\"] == 2].reset_index(drop=True)\n",
    "    train_3 = train[train[\"preprocessed_label\"] == 3].reset_index(drop=True)\n",
    "    train_4 = train[train[\"preprocessed_label\"] == 4].reset_index(drop=True)\n",
    "    train_5 = train[train[\"preprocessed_label\"] == 5].reset_index(drop=True)\n",
    "    \n",
    "    train_1_aug = concat_AEDA_sent(train_1, 1323)\n",
    "    train_2_aug = concat_AEDA_sent(train_2, 1906)\n",
    "    train_3_aug = concat_AEDA_sent(train_3, 1647)\n",
    "    train_4_aug = concat_AEDA_sent(train_4, 936)\n",
    "    train_5_aug = concat_AEDA_sent(train_5, 2750)\n",
    "\n",
    "    train_0 =  pd.concat([train_0, train_1_aug, train_2_aug, train_3_aug, train_4_aug, train_5_aug], ignore_index=True)\n",
    "    \n",
    "    auged_df = train_0.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    return auged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ca215",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = AEDA_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "36275dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('./train_auged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7135b670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
