{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb365994-9c68-40db-9a5c-086fcdbc5157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: koeda in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.4)\n",
      "Requirement already satisfied: konlpy>=0.5.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from koeda) (0.6.0)\n",
      "Requirement already satisfied: tweepy==3.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from koeda) (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.19.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from koeda) (1.20.3)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy==3.10.0->koeda) (2.26.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy==3.10.0->koeda) (1.3.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy==3.10.0->koeda) (1.16.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy>=0.5.2->koeda) (4.6.3)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy>=0.5.2->koeda) (1.4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy==3.10.0->koeda) (3.2.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy==3.10.0->koeda) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy==3.10.0->koeda) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy==3.10.0->koeda) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy==3.10.0->koeda) (3.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy==3.10.0->koeda) (1.7.1)\n",
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.23.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install koeda\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c5d1c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93847cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd2565a",
   "metadata": {},
   "source": [
    "NLP에서 Text Augmentation 방법은 크게 텍스트의 일부를 변형하여 데이터를 증강하는 방법과 생성모델을 사용하여 새로운 텍스트를 생성하여 데이터를 증강하는 방법이 있다. \n",
    "그 중에서 가장 손쉽게 접근할 수 있는 방법은 KoEDA 라이브러리를 사용하는 것이었다.\n",
    "KoEDA는 EDA와 AEDA 논문에서 소개된 방식을 한국어 Wordnet 으로 Porting하여 공개한 오픈소스 라이브러리이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76808284",
   "metadata": {},
   "source": [
    "<EDA의 네 가지 기법> \n",
    "\n",
    "Easy Data Augmentation라는 논문에서는 데이터를 다음의 네 가지 기법을 통해 자연어 데이터를 증강하고자 한다.\n",
    "\n",
    "* 유의어로 교체(Synonym Replacement, SR): 문장에서 랜덤으로 stop words가 아닌 n 개의 단어들을 선택해 임의로 선택한 동의어들 중 하나로 바꾸는 기법.\n",
    "\n",
    "* 랜덤 삽입(Random Insertion, RI): 문장 내에서 stop word를 제외한 나머지 단어들 중에서, 랜덤으로 선택한 단어의 동의어를 임의로 정한다. 그리고 동의어를 문장 내 임의의 자리에 넣는걸 n번 반복한다.\n",
    "\n",
    "* 랜덤 교체(Random Swap, RS): 무작위로 문장 내에서 두 단어를 선택하고 위치를 바꾼다. 이것도 n번 반복\n",
    "\n",
    "* 랜덤 삭제(Random Deletion, RD): 확률 p를 통해 문장 내에 있는 각 단어들을 랜덤하게 삭제한다.\n",
    "\n",
    "<EDA의 성능>\n",
    "\n",
    "데이터셋이 적다는 가정에서, 데이터셋이 500개일 때 EDA를 포함하면 평균적으로 3%의 성능이 증가함을 확인할 수 있다. full set일 때도 EDA를 사용하면 평균적으로 성능의 향상이 있었다. \n",
    "\n",
    "하지만 우리가 사용할 BERT 등의 선학습 모델은 거대 데이터셋으로 선학습되었기에 데이터셋의 개선 효과를 못 볼 수도 있다고 한다.\n",
    "\n",
    "<EDA 활용 팁>\n",
    "\n",
    "한 문장에 대해서 몇 개의 문장을 만들건지에 따라 α값에 조정이 필요하며, \n",
    "4문장 이하는 p=0.1, 4문장 초과는 p=0.05 정도의 확률값으로 데이터를 변형하는게 가장 성능이 좋았다고 저술되어있다. \n",
    "\n",
    "하지만 텍스트 데이터의 특성상, 위치를 바꾸거나 일부 단어를 제거하는 것은 결국 본 문장의 의미를 손실시키는 행위이기 때문에 AEDA 방법론이 등장하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c47f99-94e3-4faa-9581-fc50d1db7d8e",
   "metadata": {},
   "source": [
    "<AEDA란>\n",
    "AEDA는 문장을 손실시키지 않게 하기 위해 Special character를 문장 곳곳에 배치하는 방법론으로, \n",
    "역시 많은 특수문자가 들어가게 되면 성능이 떨어지기 때문에 적절한 확률값을 찾는 것이 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e55ebe6",
   "metadata": {},
   "source": [
    "***한글 자연어 처리 패키지 konlpy\n",
    "\n",
    "konlpy는 형태소 등을 알아서 분석해주는 편리한 패키지이지만, konlpy 내 클래스는 Java 기반이기 때문에 그냥 pip install konlpy로 설치할 수 없다. \n",
    "아래와 같은 과정을 거쳐야 하는데\n",
    "\n",
    "1. JAVA 설치\n",
    "   https://www.oracle.com/java/technologies/javase-downloads.html\n",
    "2. JAVA_HOME 환경변수 설정\n",
    "3. JPype 다운로드 및 설치\n",
    "   https://www.lfd.uci.edu/~gohlke/pythonlibs/#jpype\n",
    "   파이썬 버전과 맞게 다운로드해야지 cmd창에서 'not a valid wheel filename' 에러가 안 뜬다\n",
    "4. konlpy 설치\n",
    "\n",
    "참고: https://byeon-sg.tistory.com/entry/자연어-처리-konlpy-설치-오류-okt에러-already-loaded-in-another-classloader-SystemErro-1 [wave:티스토리]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "778561b6-ec63-412a-a299-e5aed971421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\programdata\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (1.20.3)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (1.4.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (4.6.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a953292c-59bc-4000-9f23-d766e43a551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy \n",
    "from konlpy.tag import Okt \n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "494077ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부친가 방에 들어가신다\n"
     ]
    }
   ],
   "source": [
    "# EDA 사용방법 (참고: https://github.com/toriving/KoEDA)\n",
    "from koeda import EDA\n",
    "\n",
    "eda = EDA(\n",
    "    morpheme_analyzer=\"Okt\", alpha_sr=0.3, alpha_ri=0.3, alpha_rs=0.2, prob_rd=0.05\n",
    ")\n",
    "\n",
    "text = \"아버지가 방에 들어가신다\"\n",
    "\n",
    "result = eda(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1106b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어머니 : 가 집을 : 나가신다\n"
     ]
    }
   ],
   "source": [
    "# AEDA 사용방법\n",
    "from koeda import AEDA\n",
    "\n",
    "aeda = AEDA(\n",
    "    morpheme_analyzer=\"Okt\", punc_ratio=0.3, punctuations=[\".\", \",\", \"!\", \"?\", \";\", \":\"]\n",
    ")\n",
    "\n",
    "text = \"어머니가 집을 나가신다\"\n",
    "\n",
    "result = aeda(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7eba15",
   "metadata": {},
   "source": [
    "sentence_1을 바꿀지, sentence_2를 바꿀지, sentence_1,2 모두 바꿀지? <br/>\n",
    "5점 경우는 2700개의 데이터를 추가로 만들고 싶다 <br/>\n",
    "\n",
    "<실험>\n",
    "\n",
    "1. 문장은 사이클로 돌리고 sent 1,2,(1,2)는 random으로 선택하자\n",
    "2. sentence 1,2 둘 다 aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbeda6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from koeda import AEDA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "def get_preprocessed_label(df):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        df (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i, \"preprocessed_label\"] = round(df.loc[i, \"label\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def concat_AEDA_sent(df_label, aug_nums):\n",
    "    \"\"\"라벨 x만 있는 데이터프레임에 aug_nums 만큼 AEDA된 데이터를 추가\n",
    "\n",
    "    Args:\n",
    "        df_label (DataFrame): 라벨 x만 있는 데이터프레임\n",
    "        aug_nums (int): 증강하고자 하는 수\n",
    "    Returns:\n",
    "        AEDA가 추가된 라벨 x의 데이터프레임\n",
    "    \"\"\"\n",
    "    np.random.seed(12)\n",
    "    #aug_idx = np.random.randint(0, 3, size=aug_nums)\n",
    "    aug_idx = [2]*aug_nums\n",
    "    dataset_idx = 0\n",
    "    aug_df_label = copy.deepcopy(df_label)\n",
    "\n",
    "    aeda = AEDA(\n",
    "        morpheme_analyzer=\"Okt\",\n",
    "        punc_ratio=0.3,\n",
    "        punctuations=[\".\", \",\", \"!\", \"?\", \";\", \":\"],\n",
    "    )\n",
    "\n",
    "    for i in range(aug_nums):\n",
    "        if dataset_idx >= len(df_label):\n",
    "            dataset_idx = 0\n",
    "\n",
    "        origin_id = df_label.iloc[dataset_idx][\"id\"]\n",
    "        origin_source = df_label.iloc[dataset_idx][\"source\"]\n",
    "        origin_sentence_1 = df_label.iloc[dataset_idx][\"sentence_1\"]\n",
    "        origin_sentence_2 = df_label.iloc[dataset_idx][\"sentence_2\"]\n",
    "        origin_label = df_label.iloc[dataset_idx][\"label\"]\n",
    "        origin_binarylabel = df_label.iloc[dataset_idx][\"binary-label\"]\n",
    "        origin_preprocessed_label = df_label.iloc[dataset_idx][\"preprocessed_label\"]\n",
    "\n",
    "        if aug_idx[i] == 0:  # sent_1만 aug\n",
    "            sent = aug_df_label.iloc[dataset_idx][\"sentence_1\"]\n",
    "            aug_sent = aeda(sent)\n",
    "            new_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"id\": [origin_id],\n",
    "                    \"source\": [origin_source],\n",
    "                    \"sentence_1\": [aug_sent],\n",
    "                    \"sentence_2\": [origin_sentence_2],\n",
    "                    \"label\": [origin_label],\n",
    "                    \"binary-label\": [origin_binarylabel],\n",
    "                    \"preprocessed_label\": [origin_preprocessed_label],\n",
    "                }\n",
    "            )\n",
    "            aug_df_label = pd.concat([aug_df_label, new_df], ignore_index=True)\n",
    "\n",
    "        elif aug_idx[i] == 1:  # sent_2만 aug\n",
    "            sent = aug_df_label.iloc[dataset_idx][\"sentence_2\"]\n",
    "            aug_sent = aeda(sent)\n",
    "            new_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"id\": [origin_id],\n",
    "                    \"source\": [origin_source],\n",
    "                    \"sentence_1\": [origin_sentence_1],\n",
    "                    \"sentence_2\": [aug_sent],\n",
    "                    \"label\": [origin_label],\n",
    "                    \"binary-label\": [origin_binarylabel],\n",
    "                    \"preprocessed_label\": [origin_preprocessed_label],\n",
    "                }\n",
    "            )\n",
    "            aug_df_label = pd.concat([aug_df_label, new_df], ignore_index=True)\n",
    "\n",
    "        else:  # sent_1과 2를 모두 aug\n",
    "            sent_1 = aug_df_label.iloc[dataset_idx][\"sentence_1\"]\n",
    "            sent_2 = aug_df_label.iloc[dataset_idx][\"sentence_2\"]\n",
    "            aug_sent_1 = aeda(sent_1)\n",
    "            aug_sent_2 = aeda(sent_2)\n",
    "            new_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"id\": [origin_id],\n",
    "                    \"source\": [origin_source],\n",
    "                    \"sentence_1\": [aug_sent_1],\n",
    "                    \"sentence_2\": [aug_sent_2],\n",
    "                    \"label\": [origin_label],\n",
    "                    \"binary-label\": [origin_binarylabel],\n",
    "                    \"preprocessed_label\": [origin_preprocessed_label],\n",
    "                }\n",
    "            )\n",
    "            aug_df_label = pd.concat([aug_df_label, new_df], ignore_index=True)\n",
    "\n",
    "        dataset_idx += 1\n",
    "\n",
    "    return aug_df_label\n",
    "\n",
    "\n",
    "def AEDA_data():\n",
    "\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    train = get_preprocessed_label(train)\n",
    "\n",
    "    train_0 = train[train[\"preprocessed_label\"] == 0].reset_index(drop=True)\n",
    "    train_1 = train[train[\"preprocessed_label\"] == 1].reset_index(drop=True)\n",
    "    train_2 = train[train[\"preprocessed_label\"] == 2].reset_index(drop=True)\n",
    "    train_3 = train[train[\"preprocessed_label\"] == 3].reset_index(drop=True)\n",
    "    train_4 = train[train[\"preprocessed_label\"] == 4].reset_index(drop=True)\n",
    "    train_5 = train[train[\"preprocessed_label\"] == 5].reset_index(drop=True)\n",
    "    \n",
    "    train_1_aug = concat_AEDA_sent(train_1, 1323)\n",
    "    train_2_aug = concat_AEDA_sent(train_2, 1906)\n",
    "    train_3_aug = concat_AEDA_sent(train_3, 1647)\n",
    "    train_4_aug = concat_AEDA_sent(train_4, 936)\n",
    "    train_5_aug = concat_AEDA_sent(train_5, 2750)\n",
    "\n",
    "    train_0 =  pd.concat([train_0, train_1_aug, train_2_aug, train_3_aug, train_4_aug, train_5_aug], ignore_index=True)\n",
    "    \n",
    "    auged_df = train_0.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    return auged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ca215",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = AEDA_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36275dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('./train_auged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee7328b",
   "metadata": {},
   "source": [
    "다음 방법은 생성모델을 활용한 방법으로 Conditional BERT Contextual Augmentation 논문에 소개되었다. \n",
    "\n",
    "기존 BERT에서는 token embedding + segment embedding + positional embedding 으로 representation을 구성하지만, \n",
    "\n",
    "conditional BERT의 경우 token embedding + label embedding + positional embedding 으로 representation을 구성하고, \n",
    "label을 부착한 상태로 데이터셋을 MLM task로 pretraining한다. \n",
    "이후에 mask token을 replace하는 것과 마찬가지로 label에 대하여 token replace를 수행한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
