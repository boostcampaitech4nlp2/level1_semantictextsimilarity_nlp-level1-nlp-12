{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb365994-9c68-40db-9a5c-086fcdbc5157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: koeda in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.4)\n",
      "Requirement already satisfied: konlpy>=0.5.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from koeda) (0.6.0)\n",
      "Requirement already satisfied: tweepy==3.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from koeda) (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.19.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from koeda) (1.20.3)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy==3.10.0->koeda) (2.26.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy==3.10.0->koeda) (1.3.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy==3.10.0->koeda) (1.16.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy>=0.5.2->koeda) (4.6.3)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy>=0.5.2->koeda) (1.4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy==3.10.0->koeda) (3.2.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy==3.10.0->koeda) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy==3.10.0->koeda) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy==3.10.0->koeda) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy==3.10.0->koeda) (3.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy==3.10.0->koeda) (1.7.1)\n",
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.23.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install koeda\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c5d1c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93847cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa477d9c",
   "metadata": {},
   "source": [
    "NLP에서 Text Augmentation 방법은 크게 텍스트의 일부를 변형하여 데이터를 증강하는 방법과 생성모델을 사용하여 새로운 텍스트를 생성하여 데이터를 증강하는 방법이 있다. \n",
    "그 중에서 가장 손쉽게 접근할 수 있는 방법은 KoEDA 라이브러리를 사용하는 것이었다.\n",
    "KoEDA는 EDA와 AEDA 논문에서 소개된 방식을 한국어 Wordnet 으로 Porting하여 공개한 오픈소스 라이브러리이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76808284",
   "metadata": {},
   "source": [
    "<EDA의 네 가지 기법> \n",
    "\n",
    "Easy Data Augmentation라는 논문에서는 데이터를 다음의 네 가지 기법을 통해 자연어 데이터를 증강하고자 한다.\n",
    "\n",
    "* 유의어로 교체(Synonym Replacement, SR): 문장에서 랜덤으로 stop words가 아닌 n 개의 단어들을 선택해 임의로 선택한 동의어들 중 하나로 바꾸는 기법.\n",
    "\n",
    "* 랜덤 삽입(Random Insertion, RI): 문장 내에서 stop word를 제외한 나머지 단어들 중에서, 랜덤으로 선택한 단어의 동의어를 임의로 정한다. 그리고 동의어를 문장 내 임의의 자리에 넣는걸 n번 반복한다.\n",
    "\n",
    "* 랜덤 교체(Random Swap, RS): 무작위로 문장 내에서 두 단어를 선택하고 위치를 바꾼다. 이것도 n번 반복\n",
    "\n",
    "* 랜덤 삭제(Random Deletion, RD): 확률 p를 통해 문장 내에 있는 각 단어들을 랜덤하게 삭제한다.\n",
    "\n",
    "<EDA의 성능>\n",
    "\n",
    "데이터셋이 적다는 가정에서, 데이터셋이 500개일 때 EDA를 포함하면 평균적으로 3%의 성능이 증가함을 확인할 수 있다. full set일 때도 EDA를 사용하면 평균적으로 성능의 향상이 있었다. \n",
    "\n",
    "하지만 우리가 사용할 BERT 등의 선학습 모델은 거대 데이터셋으로 선학습되었기에 데이터셋의 개선 효과를 못 볼 수도 있다고 한다.\n",
    "\n",
    "<EDA 활용 팁>\n",
    "\n",
    "한 문장에 대해서 몇 개의 문장을 만들건지에 따라 α값에 조정이 필요하며, \n",
    "4문장 이하는 p=0.1, 4문장 초과는 p=0.05 정도의 확률값으로 데이터를 변형하는게 가장 성능이 좋았다고 저술되어있다. \n",
    "\n",
    "하지만 텍스트 데이터의 특성상, 위치를 바꾸거나 일부 단어를 제거하는 것은 결국 본 문장의 의미를 손실시키는 행위이기 때문에 AEDA 방법론이 등장하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c47f99-94e3-4faa-9581-fc50d1db7d8e",
   "metadata": {},
   "source": [
    "<AEDA란>\n",
    "AEDA는 문장을 손실시키지 않게 하기 위해 Special character를 문장 곳곳에 배치하는 방법론으로, \n",
    "역시 많은 특수문자가 들어가게 되면 성능이 떨어지기 때문에 적절한 확률값을 찾는 것이 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ab81f9",
   "metadata": {},
   "source": [
    "***한글 자연어 처리 패키지 konlpy\n",
    "\n",
    "konlpy는 형태소 등을 알아서 분석해주는 편리한 패키지이지만, konlpy 내 클래스는 Java 기반이기 때문에 그냥 pip install konlpy로 설치할 수 없다. \n",
    "아래와 같은 과정을 거쳐야 하는데\n",
    "\n",
    "1. JAVA 설치\n",
    "   https://www.oracle.com/java/technologies/javase-downloads.html\n",
    "2. JAVA_HOME 환경변수 설정\n",
    "3. JPype 다운로드 및 설치\n",
    "   https://www.lfd.uci.edu/~gohlke/pythonlibs/#jpype\n",
    "   파이썬 버전과 맞게 다운로드해야지 cmd창에서 'not a valid wheel filename' 에러가 안 뜬다\n",
    "4. konlpy 설치\n",
    "\n",
    "참고: https://byeon-sg.tistory.com/entry/자연어-처리-konlpy-설치-오류-okt에러-already-loaded-in-another-classloader-SystemErro-1 [wave:티스토리]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "778561b6-ec63-412a-a299-e5aed971421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\programdata\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (1.20.3)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (1.4.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (4.6.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a953292c-59bc-4000-9f23-d766e43a551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy \n",
    "from konlpy.tag import Okt \n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "494077ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부친가 방에 들어가신다\n"
     ]
    }
   ],
   "source": [
    "# EDA 사용방법 (참고: https://github.com/toriving/KoEDA)\n",
    "from koeda import EDA\n",
    "\n",
    "eda = EDA(\n",
    "    morpheme_analyzer=\"Okt\", alpha_sr=0.3, alpha_ri=0.3, alpha_rs=0.2, prob_rd=0.05\n",
    ")\n",
    "\n",
    "text = \"아버지가 방에 들어가신다\"\n",
    "\n",
    "result = eda(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1106b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어머니 : 가 집을 : 나가신다\n"
     ]
    }
   ],
   "source": [
    "# AEDA 사용방법\n",
    "from koeda import AEDA\n",
    "\n",
    "aeda = AEDA(\n",
    "    morpheme_analyzer=\"Okt\", punc_ratio=0.3, punctuations=[\".\", \",\", \"!\", \"?\", \";\", \":\"]\n",
    ")\n",
    "\n",
    "text = \"어머니가 집을 나가신다\"\n",
    "\n",
    "result = aeda(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7eba15",
   "metadata": {},
   "source": [
    "sentence_1을 바꿀지, sentence_2를 바꿀지, sentence_1,2 모두 바꿀지? <br/>\n",
    "5점 경우는 2700개의 데이터를 추가로 만들고 싶다 <br/>\n",
    "\n",
    "<실험>\n",
    "\n",
    "1. 문장은 사이클로 돌리고 sent 1,2,(1,2)는 random으로 선택하자\n",
    "2. sentence 1,2 둘 다 aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbeda6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from koeda import AEDA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "def get_preprocessed_label(df):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        df (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i, \"preprocessed_label\"] = round(df.loc[i, \"label\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def concat_AEDA_sent(df_label, aug_nums):\n",
    "    \"\"\"라벨 x만 있는 데이터프레임에 aug_nums 만큼 AEDA된 데이터를 추가\n",
    "\n",
    "    Args:\n",
    "        df_label (DataFrame): 라벨 x만 있는 데이터프레임\n",
    "        aug_nums (int): 증강하고자 하는 수\n",
    "    Returns:\n",
    "        AEDA가 추가된 라벨 x의 데이터프레임\n",
    "    \"\"\"\n",
    "    np.random.seed(12)\n",
    "    #aug_idx = np.random.randint(0, 3, size=aug_nums)\n",
    "    aug_idx = [2]*aug_nums\n",
    "    dataset_idx = 0\n",
    "    aug_df_label = copy.deepcopy(df_label)\n",
    "\n",
    "    aeda = AEDA(\n",
    "        morpheme_analyzer=\"Okt\",\n",
    "        punc_ratio=0.3,\n",
    "        punctuations=[\".\", \",\", \"!\", \"?\", \";\", \":\"],\n",
    "    )\n",
    "\n",
    "    for i in range(aug_nums):\n",
    "        if dataset_idx >= len(df_label):\n",
    "            dataset_idx = 0\n",
    "\n",
    "        origin_id = df_label.iloc[dataset_idx][\"id\"]\n",
    "        origin_source = df_label.iloc[dataset_idx][\"source\"]\n",
    "        origin_sentence_1 = df_label.iloc[dataset_idx][\"sentence_1\"]\n",
    "        origin_sentence_2 = df_label.iloc[dataset_idx][\"sentence_2\"]\n",
    "        origin_label = df_label.iloc[dataset_idx][\"label\"]\n",
    "        origin_binarylabel = df_label.iloc[dataset_idx][\"binary-label\"]\n",
    "        origin_preprocessed_label = df_label.iloc[dataset_idx][\"preprocessed_label\"]\n",
    "\n",
    "        if aug_idx[i] == 0:  # sent_1만 aug\n",
    "            sent = aug_df_label.iloc[dataset_idx][\"sentence_1\"]\n",
    "            aug_sent = aeda(sent)\n",
    "            new_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"id\": [origin_id],\n",
    "                    \"source\": [origin_source],\n",
    "                    \"sentence_1\": [aug_sent],\n",
    "                    \"sentence_2\": [origin_sentence_2],\n",
    "                    \"label\": [origin_label],\n",
    "                    \"binary-label\": [origin_binarylabel],\n",
    "                    \"preprocessed_label\": [origin_preprocessed_label],\n",
    "                }\n",
    "            )\n",
    "            aug_df_label = pd.concat([aug_df_label, new_df], ignore_index=True)\n",
    "\n",
    "        elif aug_idx[i] == 1:  # sent_2만 aug\n",
    "            sent = aug_df_label.iloc[dataset_idx][\"sentence_2\"]\n",
    "            aug_sent = aeda(sent)\n",
    "            new_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"id\": [origin_id],\n",
    "                    \"source\": [origin_source],\n",
    "                    \"sentence_1\": [origin_sentence_1],\n",
    "                    \"sentence_2\": [aug_sent],\n",
    "                    \"label\": [origin_label],\n",
    "                    \"binary-label\": [origin_binarylabel],\n",
    "                    \"preprocessed_label\": [origin_preprocessed_label],\n",
    "                }\n",
    "            )\n",
    "            aug_df_label = pd.concat([aug_df_label, new_df], ignore_index=True)\n",
    "\n",
    "        else:  # sent_1과 2를 모두 aug\n",
    "            sent_1 = aug_df_label.iloc[dataset_idx][\"sentence_1\"]\n",
    "            sent_2 = aug_df_label.iloc[dataset_idx][\"sentence_2\"]\n",
    "            aug_sent_1 = aeda(sent_1)\n",
    "            aug_sent_2 = aeda(sent_2)\n",
    "            new_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"id\": [origin_id],\n",
    "                    \"source\": [origin_source],\n",
    "                    \"sentence_1\": [aug_sent_1],\n",
    "                    \"sentence_2\": [aug_sent_2],\n",
    "                    \"label\": [origin_label],\n",
    "                    \"binary-label\": [origin_binarylabel],\n",
    "                    \"preprocessed_label\": [origin_preprocessed_label],\n",
    "                }\n",
    "            )\n",
    "            aug_df_label = pd.concat([aug_df_label, new_df], ignore_index=True)\n",
    "\n",
    "        dataset_idx += 1\n",
    "\n",
    "    return aug_df_label\n",
    "\n",
    "\n",
    "def AEDA_data():\n",
    "\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    train = get_preprocessed_label(train)\n",
    "\n",
    "    train_0 = train[train[\"preprocessed_label\"] == 0].reset_index(drop=True)\n",
    "    train_1 = train[train[\"preprocessed_label\"] == 1].reset_index(drop=True)\n",
    "    train_2 = train[train[\"preprocessed_label\"] == 2].reset_index(drop=True)\n",
    "    train_3 = train[train[\"preprocessed_label\"] == 3].reset_index(drop=True)\n",
    "    train_4 = train[train[\"preprocessed_label\"] == 4].reset_index(drop=True)\n",
    "    train_5 = train[train[\"preprocessed_label\"] == 5].reset_index(drop=True)\n",
    "    \n",
    "    train_1_aug = concat_AEDA_sent(train_1, 1323)\n",
    "    train_2_aug = concat_AEDA_sent(train_2, 1906)\n",
    "    train_3_aug = concat_AEDA_sent(train_3, 1647)\n",
    "    train_4_aug = concat_AEDA_sent(train_4, 936)\n",
    "    train_5_aug = concat_AEDA_sent(train_5, 2750)\n",
    "\n",
    "    train_0 =  pd.concat([train_0, train_1_aug, train_2_aug, train_3_aug, train_4_aug, train_5_aug], ignore_index=True)\n",
    "    \n",
    "    auged_df = train_0.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    return auged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce1ca215",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = AEDA_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36275dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('./train_auged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db20e3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "      <th>binary-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>boostcamp-sts-v1-train-000</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~</td>\n",
       "      <td>반전도 있고,사랑도 있고재미도있네요.</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boostcamp-sts-v1-train-001</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>앗 제가 접근권한이 없다고 뜹니다;;</td>\n",
       "      <td>오, 액세스 권한이 없다고 합니다.</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boostcamp-sts-v1-train-002</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>주택청약조건 변경해주세요.</td>\n",
       "      <td>주택청약 무주택기준 변경해주세요.</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boostcamp-sts-v1-train-003</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>입사후 처음 대면으로 만나 반가웠습니다.</td>\n",
       "      <td>화상으로만 보다가 리얼로 만나니 정말 반가웠습니다.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boostcamp-sts-v1-train-004</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>뿌듯뿌듯 하네요!!</td>\n",
       "      <td>꼬옥 실제로 한번 뵈어요 뿌뿌뿌~!~!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9319</th>\n",
       "      <td>boostcamp-sts-v1-train-9319</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>교원능력개발평가에서 교원이 보호받을 수 있는 장치를 마련해야합니다</td>\n",
       "      <td>본인이 납부한 국민연금 금액을 기준으로 대출을 받을 수 있는 제도를 마련해 주세요</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9320</th>\n",
       "      <td>boostcamp-sts-v1-train-9320</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>여성가족부의 폐지를 원합니드</td>\n",
       "      <td>여성가족부 폐지를 청원 합니다.</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9321</th>\n",
       "      <td>boostcamp-sts-v1-train-9321</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>국회의원들 월급좀 줄여주세요</td>\n",
       "      <td>공무원 봉급좀 줄이지좀 마세요</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>boostcamp-sts-v1-train-9322</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>오늘 못한 점심은 다음에 다시 츄라이 하기로 해요!!</td>\n",
       "      <td>오늘 못먹은 밥은 꼭 담에 먹기로 하고요!!</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9323</th>\n",
       "      <td>boostcamp-sts-v1-train-9323</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>법정공휴일 휴무관련 (근로자)</td>\n",
       "      <td>법정공휴일의 유급휴무화를 막아야 합니다.</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9324 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id            source  \\\n",
       "0      boostcamp-sts-v1-train-000      nsmc-sampled   \n",
       "1      boostcamp-sts-v1-train-001         slack-rtt   \n",
       "2      boostcamp-sts-v1-train-002  petition-sampled   \n",
       "3      boostcamp-sts-v1-train-003     slack-sampled   \n",
       "4      boostcamp-sts-v1-train-004     slack-sampled   \n",
       "...                           ...               ...   \n",
       "9319  boostcamp-sts-v1-train-9319  petition-sampled   \n",
       "9320  boostcamp-sts-v1-train-9320  petition-sampled   \n",
       "9321  boostcamp-sts-v1-train-9321  petition-sampled   \n",
       "9322  boostcamp-sts-v1-train-9322     slack-sampled   \n",
       "9323  boostcamp-sts-v1-train-9323  petition-sampled   \n",
       "\n",
       "                                  sentence_1  \\\n",
       "0     스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~   \n",
       "1                       앗 제가 접근권한이 없다고 뜹니다;;   \n",
       "2                             주택청약조건 변경해주세요.   \n",
       "3                     입사후 처음 대면으로 만나 반가웠습니다.   \n",
       "4                                 뿌듯뿌듯 하네요!!   \n",
       "...                                      ...   \n",
       "9319    교원능력개발평가에서 교원이 보호받을 수 있는 장치를 마련해야합니다   \n",
       "9320                         여성가족부의 폐지를 원합니드   \n",
       "9321                         국회의원들 월급좀 줄여주세요   \n",
       "9322           오늘 못한 점심은 다음에 다시 츄라이 하기로 해요!!   \n",
       "9323                        법정공휴일 휴무관련 (근로자)   \n",
       "\n",
       "                                         sentence_2  label  binary-label  \n",
       "0                              반전도 있고,사랑도 있고재미도있네요.    2.2           0.0  \n",
       "1                               오, 액세스 권한이 없다고 합니다.    4.2           1.0  \n",
       "2                                주택청약 무주택기준 변경해주세요.    2.4           0.0  \n",
       "3                      화상으로만 보다가 리얼로 만나니 정말 반가웠습니다.    3.0           1.0  \n",
       "4                             꼬옥 실제로 한번 뵈어요 뿌뿌뿌~!~!    0.0           0.0  \n",
       "...                                             ...    ...           ...  \n",
       "9319  본인이 납부한 국민연금 금액을 기준으로 대출을 받을 수 있는 제도를 마련해 주세요    0.2           0.0  \n",
       "9320                              여성가족부 폐지를 청원 합니다.    4.2           1.0  \n",
       "9321                               공무원 봉급좀 줄이지좀 마세요    0.6           0.0  \n",
       "9322                       오늘 못먹은 밥은 꼭 담에 먹기로 하고요!!    3.2           1.0  \n",
       "9323                         법정공휴일의 유급휴무화를 막아야 합니다.    1.4           0.0  \n",
       "\n",
       "[9324 rows x 6 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b95082b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'source', 'sentence_1', 'sentence_2', 'label', 'binary-label'], dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c479ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy = train.copy()\n",
    "train['sentence_1'] = train_copy['sentence_2']\n",
    "train['sentence_2'] = train_copy['sentence_1']\n",
    "train.to_csv('./train_auged2.csv') # 실행할 때는 파일명 바꾸기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de9b0ca",
   "metadata": {},
   "source": [
    "다음 방법은 생성모델을 활용한 방법으로 Conditional BERT Contextual Augmentation 논문에 소개되었다. \n",
    "\n",
    "기존 BERT에서는 token embedding + segment embedding + positional embedding 으로 representation을 구성하지만, \n",
    "\n",
    "conditional BERT의 경우 token embedding + label embedding + positional embedding 으로 representation을 구성하고, \n",
    "label을 부착한 상태로 데이터셋을 MLM task로 pretraining한다. \n",
    "이후에 mask token을 replace하는 것과 마찬가지로 label에 대하여 token replace를 수행한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
